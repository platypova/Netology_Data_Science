{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25a8396",
   "metadata": {},
   "source": [
    "# Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4809b62b",
   "metadata": {},
   "source": [
    "### Классификация по тональности\n",
    "В этом домашнем задании вам предстоит классифицировать по тональности отзывы на банки с сайта banki.ru.\n",
    "\n",
    "Данные содержат непосредственно тексты отзывов, некоторую дополнительную информацию, а также оценку по шкале от 1 до 5.\n",
    "\n",
    "Тексты хранятся в json-ах в массиве responses.\n",
    "\n",
    "Посмотрим на примере отзыва: возьмите для удобства ноутбук, размещенный в папке репозитория.\n",
    "\n",
    "\n",
    "### Часть 1. Анализ текстов\n",
    "1. Посчитайте количество отзывов в разных городах и на разные банки\n",
    "2. Постройте гистограмы длин слов в символах и в словах\n",
    "3. Найдите 10 самых частых:\n",
    "- слов\n",
    "- слов без стоп-слов\n",
    "- лемм\n",
    "- существительных\n",
    "\n",
    "4. Постройте кривые Ципфа и Хипса\n",
    "5. Ответьте на следующие вопросы:\n",
    "- какое слово встречается чаще, \"сотрудник\" или \"клиент\"?\n",
    "- сколько раз встречается слова \"мошенничество\" и \"доверие\"?\n",
    "\n",
    "6. В поле \"rating_grade\" записана оценка отзыва по шкале от 1 до 5. Используйте меру $tf-idf$, для того, чтобы найти ключевые слова и биграмы для положительных отзывов (с оценкой 5) и отрицательных отзывов (с оценкой 1)\n",
    "\n",
    "### Часть 2. Тематическое моделирование\n",
    "1. Постройте несколько тематических моделей коллекции документов с разным числом тем. Приведите примеры понятных (интерпретируемых) тем.\n",
    "2. Найдите темы, в которых упомянуты конкретные банки (Сбербанк, ВТБ, другой банк). Можете ли вы их прокомментировать / объяснить?\n",
    "Эта часть задания может быть сделана с использованием gensim.\n",
    "\n",
    "### Часть 3. Классификация текстов\n",
    "Сформулируем для простоты задачу бинарной классификации: будем классифицировать на два класса, то есть, различать резко отрицательные отзывы (с оценкой 1) и положительные отзывы (с оценкой 5).\n",
    "\n",
    "1. Составьте обучающее и тестовое множество: выберите из всего набора данных N1 отзывов с оценкой 1 и N2 отзывов с оценкой 5 (значение N1 и N2 – на ваше усмотрение). Используйте sklearn.model_selection.train_test_split для разделения множества отобранных документов на обучающее и тестовое.\n",
    "2. Используйте любой известный вам алгоритм классификации текстов для решения задачи и получите baseline. Сравните разные варианты векторизации текста: использование только униграм, пар или троек слов или с использованием символьных $n$-грам.\n",
    "3. Сравните, как изменяется качество решения задачи при использовании скрытых тем в качестве признаков:\n",
    "- 1-ый вариант: $tf-idf$ преобразование (sklearn.feature_extraction.text.TfidfTransformer) и сингулярное разложение (оно же – латентый семантический анализ) (sklearn.decomposition.TruncatedSVD),\n",
    "- 2-ой вариант: тематические модели LDA (sklearn.decomposition.LatentDirichletAllocation).\n",
    "Используйте accuracy и F-measure для оценки качества классификации.\n",
    "\n",
    "В ноутбуке, размещенном в папке репозитория. написан примерный Pipeline для классификации текстов.\n",
    "\n",
    "Эта часть задания может быть сделана с использованием sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621cb2f",
   "metadata": {},
   "source": [
    "## Посмотрим пример отзыва"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "067d823a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import bz2\n",
    "import regex\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "import seaborn as sns\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1313c2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1cae4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polina/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913458a4ceee4c3eaf0b39e80f43ed17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "responses = []\n",
    "with bz2.BZ2File('banki_responses.json.bz2', 'r') as thefile:\n",
    "    for row in tqdm(thefile):\n",
    "        resp = json.loads(row)\n",
    "        if not resp['rating_not_checked'] and (len(resp['text'].split()) > 0):\n",
    "            responses.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e13dadd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153499"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сколько всего записей:\n",
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97e4901f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'city': 'г. Воронеж',\n",
       " 'rating_not_checked': False,\n",
       " 'title': 'Платеж через терминал Сбербанка',\n",
       " 'num_comments': 0,\n",
       " 'bank_license': 'лицензия № 1481',\n",
       " 'author': 'nadivlasova',\n",
       " 'bank_name': 'Сбербанк России',\n",
       " 'datetime': '2015-05-24 18:28:14',\n",
       " 'text': 'Добрый день!  14.05.15 я оплатила наличными через терминал Сбербанка счет проводного интернета Билайн. Все данные были введены верно, однако платеж не поступил на счет. В колл-центре Сбербанка, куда я обратилась, мне сказали подождать 3 банковских (то есть рабочих) дня. Но и в течение 3х рабочих дней платеж не был зачислен.  20.05.15 я обратилась в отделение Сбербанка (г. Воронеж, доп.офис 9013/0139, ул. Новгородская, 121), где совершала платеж. Сотрудник Людмила Н-а, принявшая у меня паспорт и чек, сказала, что проверит информацию.  В итоге я ждала 20 минут, о времени ожидания меня не предупредили, о причинах задержки тоже. Начальник доп. офиса подошла к Людмиле Н-й и сделала ей замечание, так как, видимо, она слишком долго рассматривала мой вопрос. Людмила вызвала следующего клиента, а мои документы молча передала своей коллеге. Только мой вопрос: \"Что происходит?\" заставил ее выдавить из себя, что у нее \"висит\" компьютер и посмотреть информацию по платежу она не может. Другой сотрудник, Светлана С-а, тоже долго искала информацию (еще минут 20), потом и вовсе ушла, оставив мои документы на столе. Она вспомнила про меня только когда я ее окликнула, и сказала, что платеж был проведен с ошибкой, что она передаст информацию в бухгалтерию и в течение двух дней платеж будет зачислен, а мне придет смс о рассмотрении моего заявления. В итоге два дня прошли, платеж так и не был зачислен, смс о рассмотрении  заявления тоже не приходило.',\n",
       " 'rating_grade': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример отзыва:\n",
    "responses[1234]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b2402",
   "metadata": {},
   "source": [
    "### Часть 1. Анализ текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32e0559c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "167e8212",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>rating_not_checked</th>\n",
       "      <th>title</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>bank_license</th>\n",
       "      <th>author</th>\n",
       "      <th>bank_name</th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "      <th>rating_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>г. Москва</td>\n",
       "      <td>False</td>\n",
       "      <td>Жалоба</td>\n",
       "      <td>0</td>\n",
       "      <td>лицензия № 2562</td>\n",
       "      <td>uhnov1</td>\n",
       "      <td>Бинбанк</td>\n",
       "      <td>2015-06-08 12:50:54</td>\n",
       "      <td>Добрый день! Я не являюсь клиентом банка и пор...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>г. Новосибирск</td>\n",
       "      <td>False</td>\n",
       "      <td>Не могу пользоваться услугой Сбербанк он-лайн</td>\n",
       "      <td>0</td>\n",
       "      <td>лицензия № 1481</td>\n",
       "      <td>Foryou</td>\n",
       "      <td>Сбербанк России</td>\n",
       "      <td>2015-06-08 11:09:57</td>\n",
       "      <td>Доброго дня! Являюсь держателем зарплатной кар...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>г. Москва</td>\n",
       "      <td>False</td>\n",
       "      <td>Двойное списание за один товар.</td>\n",
       "      <td>1</td>\n",
       "      <td>лицензия № 2562</td>\n",
       "      <td>Vladimir84</td>\n",
       "      <td>Бинбанк</td>\n",
       "      <td>2015-06-05 20:14:28</td>\n",
       "      <td>Здравствуйте!  Дублирую свое заявление от 03.0...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>г. Ставрополь</td>\n",
       "      <td>False</td>\n",
       "      <td>Меняют проценты комиссии  не предупредив и не ...</td>\n",
       "      <td>2</td>\n",
       "      <td>лицензия № 1481</td>\n",
       "      <td>643609</td>\n",
       "      <td>Сбербанк России</td>\n",
       "      <td>2015-06-05 13:51:01</td>\n",
       "      <td>Добрый день!! Я открыл расчетный счет в СберБа...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>г. Челябинск</td>\n",
       "      <td>False</td>\n",
       "      <td>Верните денежные средства за страховку</td>\n",
       "      <td>1</td>\n",
       "      <td>лицензия № 2766</td>\n",
       "      <td>anfisa-2003</td>\n",
       "      <td>ОТП Банк</td>\n",
       "      <td>2015-06-05 10:58:12</td>\n",
       "      <td>04.03.2015 г. взяла кредит в вашем банке, заяв...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             city  rating_not_checked  \\\n",
       "0       г. Москва               False   \n",
       "1  г. Новосибирск               False   \n",
       "2       г. Москва               False   \n",
       "3   г. Ставрополь               False   \n",
       "4    г. Челябинск               False   \n",
       "\n",
       "                                               title  num_comments  \\\n",
       "0                                             Жалоба             0   \n",
       "1      Не могу пользоваться услугой Сбербанк он-лайн             0   \n",
       "2                    Двойное списание за один товар.             1   \n",
       "3  Меняют проценты комиссии  не предупредив и не ...             2   \n",
       "4             Верните денежные средства за страховку             1   \n",
       "\n",
       "      bank_license       author        bank_name             datetime  \\\n",
       "0  лицензия № 2562       uhnov1          Бинбанк  2015-06-08 12:50:54   \n",
       "1  лицензия № 1481       Foryou  Сбербанк России  2015-06-08 11:09:57   \n",
       "2  лицензия № 2562   Vladimir84          Бинбанк  2015-06-05 20:14:28   \n",
       "3  лицензия № 1481       643609  Сбербанк России  2015-06-05 13:51:01   \n",
       "4  лицензия № 2766  anfisa-2003         ОТП Банк  2015-06-05 10:58:12   \n",
       "\n",
       "                                                text  rating_grade  \n",
       "0  Добрый день! Я не являюсь клиентом банка и пор...           NaN  \n",
       "1  Доброго дня! Являюсь держателем зарплатной кар...           NaN  \n",
       "2  Здравствуйте!  Дублирую свое заявление от 03.0...           NaN  \n",
       "3  Добрый день!! Я открыл расчетный счет в СберБа...           NaN  \n",
       "4  04.03.2015 г. взяла кредит в вашем банке, заяв...           NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54a5896a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 153499 entries, 0 to 153498\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   city                138325 non-null  object \n",
      " 1   rating_not_checked  153499 non-null  bool   \n",
      " 2   title               153499 non-null  object \n",
      " 3   num_comments        153499 non-null  int64  \n",
      " 4   bank_license        153498 non-null  object \n",
      " 5   author              153479 non-null  object \n",
      " 6   bank_name           153499 non-null  object \n",
      " 7   datetime            153499 non-null  object \n",
      " 8   text                153499 non-null  object \n",
      " 9   rating_grade        88658 non-null   float64\n",
      "dtypes: bool(1), float64(1), int64(1), object(7)\n",
      "memory usage: 10.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7039dc",
   "metadata": {},
   "source": [
    "#### 1. Посчитайте количество отзывов в разных городах и на разные банки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d71835c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Всего банков\n",
    "len(data['bank_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db4df83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Сбербанк России               26327\n",
       "Альфа-Банк                    10224\n",
       "ВТБ 24                         8185\n",
       "Русский Стандарт               7943\n",
       "Хоум Кредит Банк               7549\n",
       "                              ...  \n",
       "Академический Русский Банк        1\n",
       "Одинбанк                          1\n",
       "Камабанк                          1\n",
       "Независимый Банк Развития         1\n",
       "Кубаньторгбанк                    1\n",
       "Name: bank_name, Length: 670, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Количество отзывов по банкам:\n",
    "data['bank_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c4b4442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Количество отзывов по городам:\n",
    "#data['city'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "960cda08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Уберем строки, где поле город пустое\n",
    "data = data.dropna(subset = {'city'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca81f4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5823"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Всего вариантов в поле \"Город\":\n",
    "len(unique(data['city']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2c4308",
   "metadata": {},
   "source": [
    "Используем библиотеку natasha, чтобы нормализовать названия городов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4459a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "    DatesExtractor,\n",
    "    MoneyExtractor,\n",
    "    AddrExtractor,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c5c3c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    normals = []\n",
    "    for span in doc.spans:\n",
    "        if span.type == 'LOC':\n",
    "            span.normalize(morph_vocab)\n",
    "            normals.append(span.normal)\n",
    "    normals_str = ' '.join((str(n) for n in normals))\n",
    "    normals_str = normals_str.lower()\n",
    "    return normals_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31619495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['city_normalized'] = data['city'].apply(lambda x: normalize(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d67ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9406a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# всего \"городов\" после обработки:\n",
    "len(unique(data['city_normalized']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b1540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#отзывы по городам:\n",
    "data['city_normalized'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a7a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# посчитаем количество \"городов\" с одним отзывом:\n",
    "vals = data['city_normalized'].value_counts()\n",
    "vals = vals[vals < 2]\n",
    "len(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a953d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# всего записей:\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1776a20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(vals)/len(data)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8734689",
   "metadata": {},
   "source": [
    "Все же видно, что этой обработки недостаточно, так как могут оставаться такие варианты, как \"новосибирк москва\",\"уфа ульяновск\" и другие. Однако доля таких записей от общего количества отзывов - всего 1,26 процента. Будем считать их выбросами, и что в какой-то момент можем их не учитывать."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd11d53c",
   "metadata": {},
   "source": [
    "2. Постройте гистограмы длин слов в символах и в словах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96565d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# В символах:\n",
    "symbols = data['text'].apply(len)\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "symbols.plot(kind='hist',bins=200, grid = 'on')\n",
    "plt.xlabel('Длина отзывов в символах')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad40fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# В словах:\n",
    "words =  data['text'].str.split().apply(len)\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "words.plot(kind='hist',bins=200, grid = 'on')\n",
    "plt.xlabel('Длина отзывов в словах')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02f176",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# В словах, где меньше 2000 слов:\n",
    "words =  data['text'].str.split().apply(len)\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "words[words<2000].plot(kind='hist',bins=200, grid = 'on')\n",
    "plt.xlabel('Длина отзывов в словах')\n",
    "plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c092d1",
   "metadata": {},
   "source": [
    "Найдите 10 самых частых:  \n",
    "- слов  \n",
    "- слов без стоп-слов  \n",
    "- лемм  \n",
    "- существительных  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d1988",
   "metadata": {},
   "source": [
    "#### Слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d905720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf603da",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f83dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(text):\n",
    "    doc_string = re.sub(r'[^\\w\\s]', '', text)\n",
    "    doc_string = doc_string.lower()\n",
    "    doc = Doc(doc_string)\n",
    "    doc.segment(segmenter)\n",
    "    tokens = []\n",
    "    for token in doc.tokens:\n",
    "        tokens.append(token.text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b39e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(data['text'].apply(lambda x: extract_words(str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb56cef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6968682",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in words.iterrows(): \n",
    "    cnt.update(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ccd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt.most_common(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a888c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Cлов без стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d0509",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_ext = stopwords.words('russian') + ['это', 'наш' , 'тыс', 'млн', 'млрд', 'также', 'т', 'д', 'г']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30d46da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(parts):\n",
    "    for part in parts:\n",
    "        if part.isnumeric():\n",
    "            parts.remove(part)\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0140a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords_list):\n",
    "    try:\n",
    "        text = remove_digits(text)\n",
    "        return \" \".join([word for word in text if not word in stopwords_list])\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d6b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_clean = words.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f7d81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_clean['text_clean'] = words_clean['text'].apply(lambda x: remove_stopwords(x,stopwords_ext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2134f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8de877",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "\n",
    "for index, row in words_clean.iterrows(): \n",
    "    cnt.update(row['text_clean'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b28f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt.most_common(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f804bb37",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Леммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b083f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e04d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "\n",
    "def lemmatize(text, mystem=m):\n",
    "    try:\n",
    "        return \" \".join(m.lemmatize(text)).strip()  \n",
    "    except:\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_clean['lemmatized'] = words_clean['text_clean'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_clean['lemmatized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ee783",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "lemmas = []\n",
    "words_number = []\n",
    "\n",
    "for index, row in words_clean.iterrows():\n",
    "    lemmas.append(len(cnt))\n",
    "    words_number.append(sum(cnt.values()))\n",
    "    cnt.update(row['lemmatized'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788b9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt.most_common(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0879b45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d173d66f",
   "metadata": {},
   "source": [
    "#### Существительные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "f4cf753f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'оценка итог клиент альфабанка год кредит сумма карта условие платеж день девушка просрочка связь яндексденьги карта кредит банк продукт апрель кредит продукт шок кредит альфа отделение претензия трубка апрель год заявка кредит мотосалон заявка минианкетка сайт взнос момент вариант рассрочка салон отказ банк претензия смска претензия решение время суд развлечение голова шок пора альфабанк'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=MorphAnalyzer()\n",
    "\n",
    "def to_noun(text):\n",
    "    m=MorphAnalyzer()\n",
    "    try:\n",
    "        res = \" \".join([noun for noun in text.split() if m.parse(noun)[0].tag.POS =='NOUN'])\n",
    "        return res\n",
    "    \n",
    "    except:\n",
    "        return []\n",
    "\n",
    "to_noun(words_clean['lemmatized'].iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "984178fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-244-d3da15a5ed40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nouns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemmatized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mto_noun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-244-d3da15a5ed40>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nouns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemmatized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mto_noun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-241-d60da8f47d18>\u001b[0m in \u001b[0;36mto_noun\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_noun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMorphAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoun\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnoun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOS\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'NOUN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, result_type, units, probability_estimator_cls)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_dictionary_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopencorpora_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprobability_estimator_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mprobability_estimator_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummySingleTagProbabilityEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pymorphy2/opencorpora_dict/wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading dictionaries from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format: %(format_version)s, revision: %(source_revision)s, updated: %(compiled_at)s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pymorphy2/opencorpora_dict/storage.py\u001b[0m in \u001b[0;36mload_dict\u001b[0;34m(path, gramtab_format)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mstr_gramtab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_gramtab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgramtab_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mgramtab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr_gramtab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0msuffixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'suffixes.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pymorphy2/opencorpora_dict/storage.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mstr_gramtab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_gramtab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgramtab_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mgramtab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr_gramtab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0msuffixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'suffixes.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pymorphy2/tagset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mgrammemes_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mintern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrammemes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_grammemes_are_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammemes_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grammemes_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrammemes_tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "words_clean['nouns'] = words_clean['lemmatized'].apply(lambda x: to_noun(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in words_clean.iterrows():  \n",
    "    cnt.update(row['nouns'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt.most_common(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ac016",
   "metadata": {},
   "source": [
    "### 4. Постройте кривые Ципфа и Хипса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc45ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = sorted(list(cnt.values()),reverse = True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.plot(frequency[:100], range(100))\n",
    "grid(visible = 'True')\n",
    "plt.xlabel('Порядковый номер слова')\n",
    "plt.ylabel('Частота встречаемости слова')\n",
    "plt.title('Кривая Ципфа')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a8577",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.plot(words_number,lemmas)\n",
    "plt.xlabel('Количество разных слов')\n",
    "plt.ylabel('Количество слов в тексте')\n",
    "plt.title('Кривая Хипса')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fcc4a6",
   "metadata": {},
   "source": [
    "### Часть 2. Тематическое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa282aa",
   "metadata": {},
   "source": [
    "#### 1. Постройте несколько тематических моделей коллекции документов с разным числом тем. Приведите примеры понятных (интерпретируемых) тем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889940bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models import ldamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ae8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_theme = words_clean.copy()\n",
    "texts = [words_theme['lemmatized'].iloc[i].split() for i in range(len(words_theme))]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a29da",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5803bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# построим модель для 20 тем\n",
    "lda20 = ldamodel.LdaModel(corpus=corpus, \n",
    "                        id2word=dictionary, \n",
    "                        num_topics=20, \n",
    "                        alpha='auto', \n",
    "                        eta='auto', \n",
    "                        iterations = 20, \n",
    "                        passes = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим 3 темы:\n",
    "lda20.show_topics(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f436dc9",
   "metadata": {},
   "source": [
    "Темы:\n",
    "1. Судебные иски и влияние их на взаимодействие клиента с банком (просьбы об отсрочке и тд)\n",
    "2. Качество обслуживания клиентов сотрудниками банка\n",
    "3. Сложно интерпретировать эту тему, но, возможно, проблемы с кредитными выплатами у клиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae090ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# построим модель для 10 тем\n",
    "lda10 = ldamodel.LdaModel(corpus=corpus, \n",
    "                        id2word=dictionary, \n",
    "                        num_topics=10, \n",
    "                        alpha='auto', \n",
    "                        eta='auto', \n",
    "                        iterations = 20, \n",
    "                        passes = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим 3 темы:\n",
    "lda10.show_topics(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c1be7",
   "metadata": {},
   "source": [
    "Темы:\n",
    "1. Правовые вопросы при заключении договора с банком\n",
    "2. Возможные жалобы на время/качество работы отделений банка\n",
    "3. Отзывы на работу сотрудников банка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# построим модель для 5 тем\n",
    "lda5 = ldamodel.LdaModel(corpus=corpus, \n",
    "                        id2word=dictionary, \n",
    "                        num_topics=5, \n",
    "                        alpha='auto', \n",
    "                        eta='auto', \n",
    "                        iterations = 20, \n",
    "                        passes = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db690eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим 3 темы:\n",
    "lda5.show_topics(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f997159",
   "metadata": {},
   "source": [
    "1. Кредит в банке, кредитный договор\n",
    "2. Получение денег в банкомате и в отделении банка\n",
    "3. Сотрудник банка при работе с клинтом по вкладу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7bcf2",
   "metadata": {},
   "source": [
    "### 2. Найдите темы, в которых упомянуты конкретные банки (Сбербанк, ВТБ, другой банк). Можете ли вы их прокомментировать / объяснить? Эта часть задания может быть сделана с использованием gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb3331",
   "metadata": {},
   "outputs": [],
   "source": [
    "for theme in lda5.show_topics():\n",
    "    if theme[1].find('сбербанк')> 0:\n",
    "        print(theme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f437afe5",
   "metadata": {},
   "source": [
    "Получение, вопросы, связанные с получением денег  в банкомате Сбербанка и в отделении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for theme in lda20.show_topics():\n",
    "    if theme[1].find('сбербанк')> 0:\n",
    "        print(theme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f20b66",
   "metadata": {},
   "source": [
    "Вопросы, проблемы по смс-банкингу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3931e9ba",
   "metadata": {},
   "source": [
    "## Часть 3. Классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f39e3aa",
   "metadata": {},
   "source": [
    "Сформулируем для простоты задачу бинарной классификации: будем классифицировать на два класса, то есть, различать резко отрицательные отзывы (с оценкой 1) и положительные отзывы (с оценкой 5).\n",
    "\n",
    "1. Составьте обучающее и тестовое множество: выберите из всего набора данных N1 отзывов с оценкой 1 и N2 отзывов с оценкой 5 (значение N1 и N2 – на ваше усмотрение). \n",
    "\n",
    "2. Используйте sklearn.model_selection.train_test_split для разделения множества отобранных документов на обучающее и тестовое.  \n",
    "3. Используйте любой известный вам алгоритм классификации текстов для решения задачи и получите baseline. Сравните разные варианты векторизации текста: использование только униграм, пар или троек слов или с использованием символьных  𝑛 -грам.  \n",
    "4. Сравните, как изменяется качество решения задачи при использовании скрытых тем в качестве признаков:\n",
    "1-ый вариант:  𝑡𝑓−𝑖𝑑𝑓  преобразование (sklearn.feature_extraction.text.TfidfTransformer) и сингулярное разложение (оно же – латентый семантический анализ) (sklearn.decomposition.TruncatedSVD),\n",
    "2-ой вариант: тематические модели LDA (sklearn.decomposition.LatentDirichletAllocation). Используйте accuracy и F-measure для оценки качества классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba5f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.copy()\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b6156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_clean.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22affaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['text_clean'] = words_clean['text_clean']\n",
    "dataset['lemmatized'] = words_clean['lemmatized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4902aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a97ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56526f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(subset = {'rating_grade'})\n",
    "dataset = dataset[(dataset['rating_grade'] == 1.0) | (dataset['rating_grade'] == 5.0)]\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553430e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b54fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.rating_grade.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62966c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset['lemmatized'].values\n",
    "y = dataset.rating_grade.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db750e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_countvectorized = Pipeline(\n",
    "    [('vect', CountVectorizer()),\n",
    "     ('clf', LogisticRegression())]\n",
    ")\n",
    "\n",
    "params_cntv = {\n",
    "    'vect__analyzer': ['word','char'],\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (2, 2), (3, 3)),\n",
    "    'clf__C': np.logspace(-3,3,7),\n",
    "    'clf__penalty': ['l1','l2']  \n",
    "}\n",
    "\n",
    "scores=['accuracy', 'f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cntv = GridSearchCV(\n",
    "    clf_countvectorized, \n",
    "    param_grid=params_cntv, \n",
    "    cv=3,\n",
    "    scoring=scores,\n",
    "    refit=scores[0],\n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c35aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cntv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e85a124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de762e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa7bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
